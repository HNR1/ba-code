\section{Introduction}
Diffusion models are the latest advancement in computer vision and are used for tasks involving image enhancement, completion, denoising, and restoration, but most notably image synthesis. Popular generative models include DALL-E 2 \cite{ramesh2022hierarchical}, \href{https://www.midjourney.com}{Midjourney} and \textbf{Stable Diffusion} (SD), with the latter offering a public and free option for image generation.\\
A big challenge for these generative models remains the computational intensity when image sizes increase. The computational demands square with with the number of pixels (and tokens), due to the models reliance on a transformer backbone.\\
A promising remedy for this issue is the concept of Token Merging (ToMe) by \cite{bolya2023tomesd}. ToMe for SD reduces the number of processed tokens within the transformer by merging the similar tokens. Unlike token pruning methods, ToMe retains the original image size by unmerging the tokens after they were processed by a computational unit. Additionally, ToMe is model agnostic and requires no specific training. The authors claim: "ToMe for Stable Diffusion minimally impacts visual quality while offering up to $2 \times$ faster evaluation using $5.6 \times$ less memory" \cite{bolya2023tomesd}.\\
In this work, our goal is to replicate these findings while investigating different configurations of ToMe and their effect on SD's performance in image generation tasks.\\


All experiments were conducted on a high performance computing cluster (HPC) provided by the Heinrich-Heine-Universität Düsseldorf.\\
"Even at 60\% of tokens merged, ToMe for SD keeps the image the same (most of the time)"