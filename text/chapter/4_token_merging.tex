\section{Token Merging}
In Token Merging for Stable Diffusion\cite{bolya2023tomesd} the number of tokens is reduced by r\% by merging similar tokens before every diffusion step and unmerging them after to retain the original size of the image.\\
The tokens are partitioned into a source (\textbf{src}) and a destination (\textbf{dst}) set and the most similar tokens from the \textbf{src} set are continously merged into their \textbf{dst} counterparts until the number of tokens has reduced by \(r\)\%, with \(r\) being a hyperparameter determined by the user.\\ 
The choice of \(r\) is a trade-off between image fidelity and diffusion time as a lower amount of tokens requires a smaller computation time but more information about the image is lost in the merge process.\\
The merge can be applied in different components of the transformer (i.e., self-attn, cross-attn, mlp) to reduce their volume of the computation. 

\subsection{Merge and Unmerge algorithms}
\subsubsection*{Merging.} The Merge algorithm uses Bipartite-Soft-Matching to determine the similarity of tokens between the \textbf{src} and \textbf{dst} set. The two most similar tokens are taken and merged into a new token until the overall number of tokens has reduced by \(r\%\).\\
Two tokens with \(c\) channels \(x_1, x_2 \in \mathbb{R}^c\) would be merged into a new token \(x_{1,2}^* \in \mathbb{R}^c \) by averaging it's features, e.g. \[x_{1,2}^* = \frac{x_1 + x_2}{2}\]

\subsubsection*{Unmerging.} The Unmerge algorithm takes an originally merged token $x_{1,2}^* \in \mathbb{R}^c$ and splits it up into its original tokens $x_1', x_2' \in \mathbb{R}^c$, e.g. 
\begin{align*}
    x_1' = x_{1,2}^* \quad\quad
    x_2' = x_{1,2}^*
\end{align*}
in order to recreate the pre-merge amount of tokens.\\
This naive approach does lose information because the now unmerged tokens both have the average of their previous values, but this loss is small due their already high similarity before the merge.\\ Further exploration might yield improvements here.

\subsection{Bipartite-Soft-Matching}
Bipartite-Soft-Matching involves two main steps. First, the tensor of tokens is split into two tensors, one consisting of the \textbf{src} tokens and the other of \textbf{dst} tokens. Then, each token in the \textbf{src} set is matched with its most similar token in the \textbf{dst} set, resulting in the creation of a bipartite graph of tokens. 

\subsubsection*{Choosing src and dst}
The grid of tokens is partitioned into a grid of \(sx \times sy\) batches of tokens.\\
Within every batch one token is chosen for the \textbf{dst} set with the rest now belonging to the \textbf{src} set. The choice is either random (default) or the top left token is always chosen when the parameter \textbf{use\_rand=False} is set.\\
\\
The number of mergable tokens is limited by the choice of \(sx\) and \(sy\).
\begin{align*}
    r_{max} = 1-\frac{1}{sx*sy}
\end{align*}
That means for \(sx = 2\) and \(sy = 2\), no more than 75\% of tokens can be merged.\\
In the subsequent steps, the two most similar tokens are chosen greedily and merged into one, until \(r\%\) of tokens are gone.

\subsection{Token similarity}
While it may be tempting to call two tokens similar if the distance between their features is small, this is not necessarily optimal. The intermediate feature space in modern transformers is overparameterized.\\ 
Luckily, transformers natively solve this problem with QKV self-attention\cite{vaswani2017attention}.
Specifically, the keys (K) already summarize the information contained in each token for use in dot product similarity. Thus, we use cosine similarity between the keys of each token to determine which contain similar information\cite{bolya2023tome}.

\newpage
\subsubsection*{Computing cosine similarity}
ToMe extracts the key tensor (\(M\)) of the image and normalizes it before its \(split\) into the tensor of \textbf{src} tokens (\(S\)) and tensor of \textbf{dst} tokens (\(D\)).
\begin{lstlisting}[language=Python]
M = M / M.norm(dim=-1, keepdim=True)
S, D = split(M)
\end{lstlisting}
Continuing from there, the cosine similarity (\(scores\)) between every \textbf{src} and \textbf{dst} token is computed by taking the dot product of \(S\) and \(D\).
\begin{lstlisting}[language=Python]
scores = S @ D.transpose(-1, -2)
\end{lstlisting}
\(scores\) is a tensor with the cosine similarity between every token in \textbf{src} every token in \textbf{dst} 

\subsection{Different approaches?}
Text.

