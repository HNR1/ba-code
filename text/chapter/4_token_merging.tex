\section{Token Merging} \label{token_merging}
In Token Merging for Stable Diffusion (ToMe) by \cite{bolya2023tomesd}, the number of tokens in a transformer is reduced by \(r\%\) through merging similar tokens before every diffusion step and unmerging them afterward to retain the original size of the image.\\
The tokens are partitioned into a source (\textbf{src}) and a destination (\textbf{dst}) set, then the most similar tokens from the \textbf{src} set are continuously merged into their \textbf{dst} counterparts until the number of tokens has reduced by \(r\)\%.\\
The choice of \(r\) is a trade-off between image fidelity and diffusion time as a lower amount of tokens requires a smaller computation time but more information about the image is lost in the merge process.\\
The merge can be applied in different components of the transformer (i.e., self-attn, cross-attn, mlp) (see Fig.~\ref{fig:tome}).
\begin{figure}[!htb]
\centering
\includegraphics[width=0.55\textwidth]
{static/transformer_graphic.png}
\caption{Transformer with ToMe \cite[Fig.~2]{bolya2023tomesd}.}
\label{fig:tome}
\end{figure}



\subsection{Merge and Unmerge Algorithms}
\subsubsection*{Merging.} The Merge algorithm uses Bipartite-Soft-Matching to determine the similarity of tokens between the \textbf{src} and \textbf{dst} set. The two most similar tokens are taken and merged into a new token until the overall number of tokens has reduced by \(r\%\).\\
Two tokens \(x_1, x_2 \in \mathbb{R}^c\) would be merged into a new token \(x_{1,2}^* \in \mathbb{R}^c \), by averaging their features: \[x_{1,2}^* = \frac{x_1 + x_2}{2}\]



\subsubsection*{Unmerging.} The Unmerge algorithm takes an originally merged token $x_{1,2}^* \in \mathbb{R}^c$ and splits it up into its original tokens $x_1', x_2' \in \mathbb{R}^c$: 
\begin{align*}
    x_1' = x_{1,2}^* \quad\quad
    x_2' = x_{1,2}^*
\end{align*}
in order to recreate the pre-merge amount of tokens.\\
This naive approach does lose information, as the now unmerged tokens both have the average of their previous values. This loss is often small, due to token merging being based on their similarity.



\subsection{Bipartite-Soft-Matching}
Bipartite-Soft-Matching involves two main steps. First, the tensor of tokens is split into two tensors, one consisting of the \textbf{src} tokens and the other of \textbf{dst} tokens. Then, each token in the \textbf{src} set is matched with its most similar token in the \textbf{dst} set, creating a bipartite graph, with the edges between every \textbf{src} token and their closest match in the \textbf{dst} set. 



\subsubsection*{Choosing src and dst}
The grid of tokens is partitioned into a grid of \(sx \times sy\) batches of tokens.
Within every batch one token is chosen for the \textbf{dst} set (blue) with the rest now belonging to the \textbf{src} set (red). The choice is random by default, alternatively the top left token is always chosen when the parameter \textbf{use\_rand=False} is set (see Fig.~\ref{fig:src-dst}).
\begin{figure}[!htb]
\centering
\includegraphics[width=0.75\textwidth]
{static/src_dst_part.png}
\caption{Partitioning of tokens into \textbf{src} and \textbf{dst} \cite[Fig. 5]{bolya2023tome}}
\label{fig:src-dst}
\end{figure}\\
The number of mergeable tokens, which corresponds to the size of the \textbf{src} set, is limited by the choice of \(sx\) and \(sy\).
\begin{align*}
    r_{max} = 1-\frac{1}{sx*sy}
\end{align*}
That means for \(sx = 2\) and \(sy = 2\), no more than 75\% of tokens can be merged.\\
In the subsequent steps, the two most similar tokens are chosen greedily and merged into one, until \(r\%\) of tokens are gone.



\subsection{Token Similarity}
It is also relevant to define what "token similarity" exactly means. The similarity of tokens can not be determined by measuring the distance between their features, as the intermediate feature space of transformers is usually overparameterized.
For that reason, a different similarity metric is necessary. \cite{bolya2023tome} tried to utilize that the keys of the transformers self-attention summarize the information of each token and therefore determined similarity by computing the cosine similarity between the keys of each token. ToMe for SD though, moves away from proportional attention and computes the cosine similarity between the \textbf{src} and \textbf{dst} set created from the inputs to the block \(x\), instead of the aforementioned keys. The similarities are only computed once at the beginning of each transformer block.



%\newpage
\subsubsection*{Computing cosine similarity}
The token tensor (\(M\)) is normalized and then \(split\) into two tensors (\(S\) and \(D\)), separating the \textbf{src} and \textbf{dst} tokens.
Continuing from there, the cosine similarity (\(scores\)) between every \textbf{src} and \textbf{dst} token is computed by taking the dot product of \(S\) and \(D\).
\begin{lstlisting}[language=Python]
M = M / M.norm(dim=-1, keepdim=True)
S, D = split(M)
scores = S @ D.transpose(-1, -2)
\end{lstlisting}
The \(r\) most similar tokens of \textbf{src} and \textbf{dst} can now be identified by the indices of the largest values of the \(scores\) tensor.



\subsection{Additional Technical Information}
ToMe, in its basic form, only merges tokens in the self-attn block and defaults to using \(2 \times 2\) batches for token partitioning, with a randomly chosen \textbf{dst} token for every batch, resulting in a 75\% \textbf{src} and 25\% \textbf{dst} split. \\
ToMe is also not applied in every network block, but only the ones with the most tokens. Bolya and Hoffman argued:
"We try restricting ToMe to only blocks with some minimum number of tokens and find that only the blocks with the most tokens need ToMe applied to get most of the speed-up" \cite{bolya2023tomesd}.\\
Lastly, \(r\) remains consistent throughout the whole image generation process, as Bolya and Hoffman found out that merging more tokens during earlier diffusion steps and fewer during later steps does not provide a significant improvement to performance.



\subsubsection*{Image inconsistencies}
The usage of ToMe curiously prevents exact reproduction of the subsequent experiments, as minor randomness is involved in the image generation, even when \textbf{use\_rand\\=False} is set. We were not able to determine why this randomness occurs. The
variances are detectable by FID, but invisible to the human eye.
