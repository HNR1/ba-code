\section{Experimental Procedures} \label{experiments}
We perform several experiments involving the creation of image datasets using the model \href{https://huggingface.co/runwayml/stable-diffusion-v1-5}{Stable Diffusion v1.5} by runwayml \cite{Rombach_2022_CVPR} and applying ToMe in different ways, then assessing the performance.
The performance is defined by\\ 
\(1)\) speed: the average diffusion time for every image of the set and\\
\(2)\) image quality: the FID-value between sets of images that had token merging applied and their counterparts (that is same prompt, seed and image size) that didn't use any token merging.



\subsection{Setup}
\subsubsection*{Creating image datasets}
The experiments always compare how diffusion time and image quality change across a spectrum of different volumes of tokens \((r)\) removed.
The images were generated with a "DiffusionPipeline" from HuggingFace's diffusers library \cite{von-platen-etal-2022-diffusers}, using the Stable Diffusion v1.5 model.\\
We sampled multiple sets of 500 prompts from the dataset \href{https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts}{"Stable-Diffusion-Prompts"} on HuggingFace which has 80,000 prompts filtered and extracted from the image finder for Stable Diffusion: \href{https://lexica.art}{Lexica.art}, and generated a corresponding set of random seeds.
Prompts appearing multiple times within a set was not specifically prevented, due to the extremely low probability of a specific prompt-seed-pair appearing multiple times within a dataset. Same prompts with different seeds result in different images and therefore do not corrupt the validity of a dataset by our assessment.\\
Multiple images were created per prompt, with a 0\%, 10\%, 20\%, 30\%, 40\%, 50\%, and 60\% (if possible) merge applied respectively, creating a set of 3,500 (or sometimes 3,000) images which can be split up into subsets of 500 images each for every merge volume.



\subsubsection*{Measuring results}
For image quality, the FID between a subset with \(r > 0\%\) and the subset with \(r = 0\%\) of the same dataset is computed to examine how ToMe performs with different values for \(r\).\\
The speed of image generation is assessed by recording the time taken to create each individual image (in s/im) and then calculating the average across all subsets.
\begin{lstlisting}[language=Python]
start = time.time()
image = pipeline(prompt, x, y, ...)
end = time.time()
time = end - start
\end{lstlisting}



\subsubsection*{Hardware}
All experiments were conducted on a high performance computing cluster (HPC) provided by the Heinrich-Heine-Universität Düsseldorf.
GeForce GTX 1080 Ti GPUs by Nvidia were used for creating the image datasets. Individual images were always generated on a single GPU.



\subsection{Adjustments}
\subsubsection*{FID}
We use our \href{https://github.com/HNR1/pytorch_fid}{own fork} of pytorch-fid \cite{Seitzer2020FID} for FID calculation, in order to accommodate for the HPC not being connected to the internet and therefore not being able to download the weights of the Inception model. Our version loads these weights from a local directory to avoid any connection to the internet and requires the user to have them pre-installed. Some path variables also needed adjustment for usage on the HPC.



\subsubsection*{Prompts}
We shortened every prompt that exceeds 300 characters, in order to ensure that CLIP's token limit is not breached, as CLIP can only handle up to 77 tokens.
This is accomplished by determining the index (\(idx\)) of the last comma in the first 300 characters of every oversized prompt and then cutting off everything from this point onwards.
\begin{lstlisting}[language=Python]
prompt = prompt[:idx]
\end{lstlisting}


%\newpage
\subsection{Comparison to Original Setup}
\cite{bolya2023tomesd} also used the Stable Diffusion v1.5 model to generate their images. They created 2,000 $512 \times 512$ images (2 per ImageNet-1k class \cite{deng2009imagenet}) and then computed the FID values between their images and a class-balanced selection of 5,000 images from the ImageNet-1k dataset using pytorch-fid \cite{Seitzer2020FID}. Their images were generated with 50 diffusion steps and a cfg scale of 7.5, matching our setup. Speed measurements were taken by averaging the diffusion time over all 2,000 samples. All their images were created on a single 4090 GPU.\\
\\
%\newpage
The most notable difference to our setup is that we compare a set of merged images with their own unmerged versions, while Bolya and Hoffman's approach compares image sets of the same categories but does not directly try to measure alterations of images created by ToMe. Their GPU hardware also differs from ours, resulting in overall faster diffusion times in their experiments. The larger number of images in their trials also reduces the impact of FID's bias and leads to their results consistently showing lower FID values.\\
Additionally, Bolya and Hoffman also measured and analyzed the memory usage during the diffusion process, while we left memory consumption completely out of the scope of this work.
\begin{table}[!htb]
\centering
\begin{tabular}{c c c}
    \includegraphics[width=0.3\linewidth]{static/sample_imgs/secondary/wizard_0.png} & \includegraphics[width=0.3\linewidth]{static/sample_imgs/secondary/wizard_20.png} &
    \includegraphics[width=0.3\linewidth]{static/sample_imgs/secondary/wizard_50.png}\\
    \(r=0\%\) & \(20\%\) & \(50\%\) \\
\end{tabular}
\caption{$768 \times 768$ images created with the default configuration ToMe}
\end{table}


%\newpage
\subsection{Results}
\input{chapter/5_results}



%\newpage
\subsection{Comparison to Original Results}
The great differences in experimental setups and hardware make it impossible to compare specific numbers. The general promises of ToMe for SD by Bolya and Hofmann can be confirmed though, as our results show that ToMe makes image generation up to $2 \times$ faster while often maintaining great image quality even at \(r = 60\%\). Though complex prompts and images were more susceptible to deterioration when large amounts of tokens were merged, which in some cases lead to images completely falling apart. Image complexity and detail found little to no consideration in Bolya and Hoffman's work.\\
\\
However, our findings suggest different configurations of ToMe yield better results.
Firstly, we found out that expanding token merging from only the self-attn layer to the cross-attn layer improves both speed and quality. There was no data for this setup listed by Bolya and Hofmann, although they mentioned: "Note that FID does not consider prompt adherence, which is likely why merging the cross-attn module actually reduces FID" \cite{bolya2023tomesd}. \\
The implicit assertion that extending token merging to the cross-attn module lowers image quality outside of the scope of FID seems less plausible, as our setup indirectly considers prompt adherence by measuring the similarity between identical images (same size, prompt and seed), instead of larger sets of images that are merely class-balanced.\\
Though our results do confirm Bolya and Hofmann's observation that token merging in the mlp layer is clearly detrimental to image quality and thus not recommended.\\
\\
Furthermore, our findings regarding token partitioning clearly contradict Bolya and Hofmann's. Their partition experiments found notably stronger deteriorations to image quality for $1 \times 2$ strides than for $2 \times 2$ strides, with FID being almost 10\% larger at \(r=50\%\) for the former \cite[Tab.~2 (a)]{bolya2023tomesd}. For us on the other hand, FID was about 2\% smaller at \(r=50\%\) and almost 10\% smaller at \(r=30\%\) for the $1 \times 2$ batches (see Tab.~\ref{table:exp_4}). 
