\section{Background}
Stable diffusion operates in the compressed latent spaces compared to older models operating in larger image spaces.

\subsection{What are tokens?}
In image synthesis, a token is a block of pixels that are processed collectively by the transformer. Stable Diffusion uses the CLIP tokenizer\cite{radford2021learning} which defines a token as an $8 \times 8$ block of pixels, so an $768 \times 768$ pixel image has \(\frac{768*768}{8*8}=9216\) tokens.

\subsection{Stable Diffusion}
Diffusion models, which are built from a hierarchy of
denoising autoencoders. A neural network is trained to denoise images blurred with Gaussian noise by learning to reverse the diffusion process

\subsubsection{Transformer Architecture}
Different components: Self-Attention (self-attn), Cross-Attention (cross-attn) and Multilayer Perceptron (mlp).

\subsubsection{Attention and Self-Attention}
Text.

\subsubsection{Training?}
Forward diffusion. Noise is added to an image. NN trained on it.

\subsection{Frechet-Inception-Distance (FID)}
PyTorch implementation\cite{Seitzer2020FID}

\subsubsection{Inception Model}
Text.

\subsubsection{Caveats}
Inception model is biased. Inception model compresses images to 299x299 pixels. FID inconsistent with ToMe applied (not exactly reproducable)