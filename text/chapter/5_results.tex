The baseline is to look at the effects of ToMe (with default settings) applied to $768 \times 768$ images. The experiments can be split up into three different parts inspecting how ToMe affects performance\\ \(1)\) when applied to different parts of the transformer,\\ \(2)\) when applied to smaller $512 \times 512$ images, and\\ \(3)\) when different settings for partitioning the \textbf{src} and \textbf{dst} sets are used.\\
\\
The results of every experimental unit are presented in two figures. The first figure focuses on image quality, displaying FID values, while the subsequent one emphasizes image generation speed by showcasing the average time required for image generation.
Both figures have \(r\%\) on the x-axis showing how the metrics change with an increasing amount of tokens merged.

\subsubsection*{1): Experimenting with different components of the transformer}
ToMe's default configuration involves token merging solely within the self-attention module.
Our experiment aims to gauge how the performance metrics are affected by extending token merging to different combinations of transformer components.\\
All image sets were created with the same set of prompts and seeds (unless stated otherwise) to ensure the comparability of the results.



\subsubsection*{1.1): default (only self-attn) vs all (self-attn, cross-attn and mlp)}
The first experiment of part 1 compares the default setting of ToMe where token merging is only applied in the self-attn layer (black) with a different configuration that has token merging applied in the self-attn, cross-attn and mlp layer (red). 
The naive idea here is to improve performance by using token merging in every transformer layer.
\begin{figure}[!htb]
\label{fig:exp_1_1}
   \input{plots/plot1_fid}
   \input{plots/plot1_time}
\caption{FID and time for 1.1)}
\label{fig:exp_1_1}
\end{figure}\\
%\newpage
This seemingly does not yield significant improvements as image generation speed does narrowly decrease by up to 2.1 s/im (this time-delta is below 1 s/im for $r<40\%$ though; see Tab.~\ref{table:exp_1_1}), albeit at a clear cost of image quality with FID being consistently larger for this configuration. \\
ToMe in its default setup consistently produces images closer to their no-ToMe original (see Fig.~\ref{fig:exp_1_1}), so extending token merging to both the cross-attn and mlp layer does not appear beneficial. This naive approach can therefore be disregarded.



%\newpage
\subsubsection*{1.2): [default] vs [self-attn \& cross-attn] vs [self-attn \& mlp]}
After \(1.1\), the question remains whether performance drops were caused by using ToMe in both the cross-attn and mlp layer or only one of them, so the second experiment of part 1 attempts to delve deeper into that matter. This time token merging is only extended to either the cross-attn (red) or the mlp module (blue). Then a comparison is made with the results of the default ToMe settings (black) from the previous trial.
\begin{figure}[!htb]
    \input{plots/plot2_fid}
    \input{plots/plot2_time}
\caption{FID and time for 1.2)}
\label{fig:exp_1_2}
\end{figure}\\
\newpage
This time it's clearly visible that merging tokens within the self-attn and cross-attn module performs the best, both in terms of image quality and image generation speed, notably surpassing the default configuration established by \cite{bolya2023tomesd} (see Fig.~\ref{fig:exp_1_2}).\\
Token merging in the self-attn and mlp module on the other hand noticeably worsens the image quality across the board as well as the generation speed when $r<40\%$ compared to the default (see Tab.~\ref{table:exp_1_2}).\\
We can therefore conclude that extending token merging to the cross-attn module has positive effects on the performance, while extending it to the mlp module has the opposite effect.



%\newpage
\subsubsection*{1.3): [default] vs [cross-attn \& mlp] vs [only cross-attn]}
The common denominator of the previous examinations was the application of token merging in the self-attn layer. This time we are explicitly avoiding token merging in the self-attn module and instead apply it in only the cross-attn (blue) and both the cross-attn and mlp component (red).
The results of the default ToMe settings (black) again carry over from the previous trial.\\
\begin{figure}[!htb]
    \input{plots/plot5_fid}
    \input{plots/plot5_time}
\caption{FID and time for 1.3)}
\label{fig:exp_1_3}
\end{figure}\\
The most striking result here is that no token merging in the self-attn layer corresponds to no image generation speedup at all, rather slowing the process down (see Tab.~\ref{table:exp_1_3}). 
The apparent improvements to image quality compared to the ToMe default (see Fig.~\ref{fig:exp_1_3}) consequently become negligible without any speed benefits.\\
We conclusively derive that ToMe without involvement of the self-attn layer does not accelerate the image generation process at all and can be considered redundant and therefore be disregarded.




%\newpage
\subsubsection*{1.4): [default] vs [self-attn \& cross-attn] (the second time)}
The most prominent takeaway of the first three experiments is that token merging in both the self-attn and cross-attn layers improves the performance of ToMe both in terms of image quality and image generation speed.\\
We want to further examine this discovery by repeating the experiment with a new set of 500 different prompt-seed pairs and then average the results of both trials.
\begin{figure}[!htb]
    \input{plots/plot3_fid}
    \input{plots/plot3_time}
\caption{FID and time for 1.4)}
\label{fig:exp_1_4}
\end{figure}\\
%\newpage
It is again clearly visible that token merging only in the self-attn module (black) is outperformed by expanding it to the cross-attn module (red) as well (see Fig.~\ref{fig:exp_1_4}). This improvement consistently appears with every \(r\), with the gap slightly widening for larger values of \(r\) (see Tab.~\ref{table:exp_1_4}).\\
This motivates the use of token merging in both the self-attn and cross-attn modules as the new default for $768 \times 768$ images going forward.



%\newpage
\subsubsection*{2): Exploring smaller images sizes}
In this section, we want to expand the scope of our trials to smaller image sizes. Precisely, we want to repeat looking at token merging in the self-attn layer (black) and both the self-attn and cross-attn layer (red), but this time with smaller $512 \times 512$ images. Again a set of 500 prompts is randomly sampled and a corresponding set of random seeds is generated.
\begin{figure}[!htb]
    \input{plots/plot4_fid}
    \input{plots/plot4_time}
\caption{FID and time for 2)}
\label{fig:exp_2}
\end{figure}\\
%\newpage
The success of \(1.4\) does not repeat, as this time it was consistently slower to create images with token merging in the cross-attn block (see Fig.~\ref{fig:exp_2}). The FID seems to bounce a bit with the default performing slightly better for \(r=10\%\) and \(r=40\%\), but performing worse the rest of the time (see Tab.~\ref{table:exp_2}). \\
The overall speed gains are considerably smaller compared to the \(768 \times 768\) images. Even at \(r=60\%\), the speed gain is only about 25\% and the image generation process actually becomes slower by about 1 s/im at \(r=10\%\) (see Tab.~\ref{table:exp_2}).\\
We conclude that using ToMe for such small images (e.g. $512 \times 512$ or smaller) is less practical than it is for larger images. It is especially inadvisable to be used with a small merge volume (\(r<20\%\)), as there is no time gained in the diffusion process, but information is nonetheless lost during merging. \\
In consequence, we will not investigate the usage of ToMe on this scale any further.



%\newpage
\subsubsection*{3): Exploring different batch sizes}
The third part of the experimental section examines, how different values for \(sx\) and \(sy\) influence the performance of ToMe on $768 \times 768$ images. As a reminder, \(sx\) and \(sy\) determine the size of the token batches, which are used to spread the \textbf{dst} tokens somewhat evenly across the image, with larger batches resulting in a less consistent distribution. Our new default of ToMe with token merging applied in both the self-attn and cross-attn layer will be used throughout this section.



\subsubsection*{3.1): $3 \times 3$ batches}
The first choice was scaling up from $2 \times 2$ (black) to $3 \times 3$ batches (red). This setup strongly tilts the ratio of src and dst tokens toward the former and theoretically allows for a larger number of tokens to be merged. 
\begin{align*}
    r_{max} = 1-\frac{1}{3*3} = \frac{8}{9} \approx 88.89\%
\end{align*}
We will, nevertheless, not exceed a merge rate of \(r=60\%\), as image quality already exhibits clear deterioration at that level and also due to the absence of other data available for comparison.



\subsubsection*{3.2): $1 \times 2$ vertical batches}
Scaling down the same way to $1 \times 1$ batches is impossible, as it would result in every token landing in the \textbf{dst} set and \(r_{max}=0\%\).\\
We therefore chose to cut the batches in half horizontally and create an even 50-50 split between \textbf{src} and \textbf{dst}.
\begin{align*}
    r_{max} = 1-\frac{1}{1*2} = 50\%
\end{align*}
This limits our merge volume to 50\% but allows for better matches during the merge process. Compared to the $2 \times 2$ default, every \textbf{src} token now has double the number of \textbf{dst} tokens to be matched with.\\
\begin{figure}[!htb]
    \input{plots/plot6_fid}
    \input{plots/plot6_time}
\caption{FID and time for 3)}
\label{fig:exp_3}
\end{figure}\\
The resizing of the batches has only minor effects on image generation speed, as all three configurations consistently remain within one second of each other (see Tab.~\ref{table:exp_3}).\\
Image quality shows a noticeable improvement with decreasing batch sizes. The ToMe version with $3 \times 3$ batches is slightly surpassed by the $2 \times 2$ version, and this improvement is further enhanced when transitioning to $1 \times 2$ batches. (see Fig.~\ref{fig:exp_3}).
The gap closes towards \(r=50\%=r_{max}\), as every \textbf{src} token has to be merged when using $1 \times 2$ batches, regardless of how good of a match from \textbf{dst} is available.\\
It can still be concluded that using the $1 \times 2$ batches to create an equal number of \textbf{src} and \textbf{dst} tokens yields the best performance, especially when \(r\) is not close to \(0\%\) or \(r_{max}\), while the usage of larger batches with a smaller \(\textbf{dst\%}\) is not advisable.



\newpage
\subsubsection*{4): Putting it all together}
Finally, we are comparing the most successful configurations from the past experiments. That means we are looking at \textbf{setup 1}: the ToMe default by \cite{bolya2023tomesd} (black), \textbf{setup 2}: our new default with token merging expanded to the cross-attn layer from \(1.2\) and \(1.4\) (red), and \textbf{setup 3}: the new default but with $1 \times 2$ batches for token partitioning from \(3.2\) (blue).\\
We expanded the image sets for the first two configurations to 1000 images per merge volume in \(1.4\), so we will do the same for the other one to enable examinations on a larger scale.
\begin{figure}[!htb]
    \input{plots/plot7_fid}
    \input{plots/plot7_time}
\caption{FID and time for 4)}
\label{fig:exp_4}
\end{figure}\\
Again, speed is very close across the board, as \textbf{setup 2} is the fastest by only a small margin (see Tab.~\ref{table:exp_4}).
Our discoveries regarding image quality are also reinforced, with \textbf{setup 3} noticeably outperforming the rest and \textbf{setup 1} consistently being the worst (see Fig.~\ref{fig:exp_4}).\\



%\newpage
\subsubsection*{Summary}
The experiments we conducted show that ...\\
---cross-attn schneller in 768x768 aber langsamer in 768x768---