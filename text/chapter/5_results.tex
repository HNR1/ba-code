The experiments can be split up into three different parts inspecting how ToMe affects performance\\ \(1)\) when applied to different parts of the transformer,\\ \(2)\) when applied to smaller images, and\\ \(3)\) when different settings for partitioning the \textbf{src} and \textbf{dst} sets are used.\\
\\
The results of every experimental unit are presented in two figures. The first figure focuses on image quality, displaying FID values, while the subsequent one emphasizes image generation speed by showcasing how the average time required for image generation of each subset compares to the \(r=0\%\) subset.
Both figures have \(r\%\) on the x-axis showing how the metrics change with an increasing amount of tokens merged.\\
Datasets that are compared in an experimental unit, were always created with the same set of prompts and seeds, to ensure the comparability of the results.



\subsubsection*{1): Experimenting with different components of the transformer}
ToMe's default configuration involves token merging solely within the self-attention module.
Our first set of experiments aims to gauge how the performance metrics are affected when creating sets of $768 \times 768$ images, by extending token merging to different combinations of transformer components.



\subsubsection*{1.1): default (only self-attn) vs all (self-attn, cross-attn and mlp)}
The first experiment of part 1 compares the default setting of ToMe where token merging is only applied in the self-attn layer (black) with a different configuration that has token merging applied in the self-attn, cross-attn and mlp layer (red). 
\begin{figure}[!htb]
\label{fig:exp_1_1}
   \input{plots/plot1_fid}
   \input{plots/plot1_time}
\caption{FID and relative time compared to r=0\% for 1.1)}
\label{fig:exp_1_1}
\end{figure}\\
%\newpage
Firstly, the default setup of ToMe does make inference up to \(2 \times\) faster for \(r = 50\%\) (see Tab.~\ref{table:exp_1_1}), matching Bolya and Hoffman's description.\\
The naive approach to improve performance by using token merging in every transformer layer, seemingly does not yield significant improvements as image generation speed noticeably increases when \(r > 30\%\), albeit at a clear cost of image quality with FID being consistently larger for this configuration (see Fig.~\ref{fig:exp_1_1}). \\
ToMe, in its default setup, consistently produces images closer to their no-ToMe original, so extending token merging to both the cross-attn and mlp layer does not appear beneficial. This approach can therefore be disregarded.



%\newpage
\subsubsection*{1.2): [default] vs [self-attn \& cross-attn] vs [self-attn \& mlp]}
After \(1.1\), the question remains whether performance drops were caused by using ToMe in both the cross-attn and mlp layer or only one of them, so the second experiment of part 1 attempts to delve deeper into that matter. This time token merging is only extended to either the cross-attn (red) or the mlp module (blue). Then a comparison is made with the results of the default ToMe settings (black) from the previous trial.
\begin{figure}[!htb]
    \input{plots/plot2_fid}
    \input{plots/plot2_time}
\caption{FID and relative time compared to r=0\% for 1.2)}
\label{fig:exp_1_2}
\end{figure}\\
%\newpage
This time it's clearly visible that merging tokens within the self-attn and cross-attn module performs the best, as it slightly improves the ToMe defualt both in terms of image quality and image generation speed when \(r > 30\%\) (see Fig.~\ref{fig:exp_1_2}).\\
Token merging in the self-attn and mlp module on the other hand might offer the biggest speedup for image generation, but clearly performs the worst in terms of image quality across the board (see Tab.~\ref{table:exp_1_2}).\\
We can therefore conclude that extending token merging to the cross-attn module might have positive effects on the performance, while extending it to the mlp module clearly worsens image quality.



%\newpage
\subsubsection*{1.3): [default] vs [cross-attn \& mlp] vs [only cross-attn]}
The common denominator of the previous examinations was the application of token merging in the self-attn layer. This time we are explicitly avoiding token merging in the self-attn module and instead apply it in only the cross-attn (blue) and both the cross-attn and mlp component (red).
The results of the default ToMe settings (black) again carry over from the previous trial.\\
\begin{figure}[!htb]
    \input{plots/plot3_fid}
    \input{plots/plot3_time}
\caption{FID and relative time compared to r=0\% for 1.3)}
\label{fig:exp_1_3}
\end{figure}\\
The most striking result here is that no token merging in the self-attn layer corresponds to no image generation speedup at all, rather slowing the process down (see Tab.~\ref{table:exp_1_3}). 
The apparent improvements to image quality compared to the ToMe default (see Fig.~\ref{fig:exp_1_3}) consequently become negligible without any speed benefits, though it can be noted that token merging in only the cross-attn layer greatly outperforms token merging in both cross-attn and mlp layer in terms of image quality.\\
We conclusively derive that ToMe without involvement of the self-attn layer does not accelerate the image generation process at all and can be considered redundant and therefore be disregarded. It can additionally be derived from \(1.1 - 1.3\) that token merging in the mlp layer has strong negative effects on image quality and, thus, can not be recommended.




%\newpage
\subsubsection*{1.4): [default] vs [self-attn \& cross-attn] (the second time)}
The most prominent takeaway of the first three experiments is that token merging in both the self-attn and cross-attn layers improves the performance of ToMe both in terms of image quality and image generation speed.\\
We want to further examine this discovery by repeating the experiment with a new set of 500 different prompt-seed pairs and then average the results of both trials.
\begin{figure}[!htb]
    \input{plots/plot4_fid}
    \input{plots/plot4_time}
\caption{FID and relative time compared to r=0\% for 1.4)}
\label{fig:exp_1_4}
\end{figure}\\
%\newpage
It is again visible that the use of ToMe in both the self-attn and cross-attn layer, firstly, improves image quality in any case and improves inference speed when \(r > 30\%\) compared to the ToMe default (see Fig.~\ref{fig:exp_1_4}). Image generation speed seems unaffected by ToMe when \(r \leq 30\%\) (see Tab.~\ref{table:exp_1_4}).
These results motivate the use of token merging in both the self-attn and cross-attn modules as the new default for $768 \times 768$ images going forward.



%\newpage
\subsubsection*{2): Exploring smaller images sizes}
In this section, we want to expand the scope of our trials to smaller image sizes. Precisely, we want to repeat looking at token merging in the self-attn layer (black) and both the self-attn and cross-attn layer (red), but this time with smaller $512 \times 512$ images. Again a set of 500 prompts is randomly sampled and a corresponding set of random seeds is generated.
\begin{figure}[!htb]
    \input{plots/plot5_fid}
    \input{plots/plot5_time}
\caption{FID and relative time compared to r=0\% for 2)}
\label{fig:exp_2}
\end{figure}\\
%\newpage
Generally, the speedup provided by ToMe noticeably decreases to ~25\% from ~50\% in the previous trials with \(768 \times 768\) images. ToMe even worsens image generation speed when \(r<20\%\), as the image generation process actually becomes slower by about 1 s/im at \(r=10\%\) (see Tab.~\ref{table:exp_2}).\\
The inclusion of the cross-attn layer provides a consistent time improvement, but image quality gains are quite inconsistent.
ToMe in self- and cross-attn yields better quality for \(r>10\%\) but the gap closes completely at \(r=40\%\) (see Fig.~\ref{fig:exp_2}).\\
We conclude that using ToMe for such small images (e.g. $512 \times 512$ or smaller) is less practical than it is for larger images. It is especially inadvisable to be used with a small merge volume \((r<20\%)\), as there is no time gained in the diffusion process, but information is lost during merging. \\
In consequence, we will not investigate the usage of ToMe on this scale any further. Neither will we investigate ToMe's effect on larger images due to hardware limitations.



%\newpage
\subsubsection*{3): Exploring different batch sizes}
The third part of the experimental section examines, how different values for \(sx\) and \(sy\) influence the performance of ToMe on $768 \times 768$ images. As a reminder, \(sx\) and \(sy\) determine the size of the token batches, which are used to spread the \textbf{dst} tokens somewhat evenly across the image, with larger batches resulting in a less consistent distribution. Our new default of ToMe with token merging applied in both the self-attn and cross-attn layer will be used throughout this section.



%\newpage
\subsubsection*{3.1): $3 \times 3$ batches}
The first choice was scaling up from $2 \times 2$ (black) to $3 \times 3$ batches (red). This setup strongly tilts the ratio of src and dst tokens toward the former and theoretically allows for a larger number of tokens to be merged. 
\begin{align*}
    r_{max} = 1-\frac{1}{3*3} = \frac{8}{9} \approx 88.89\%
\end{align*}
We will, nevertheless, not exceed a merge rate of \(r=60\%\), as image quality already exhibits clear deterioration at that level and also due to the absence of other data available for comparison.



\subsubsection*{3.2): $1 \times 2$ vertical batches}
Scaling down the same way to $1 \times 1$ batches is impossible, as it would result in every token landing in the \textbf{dst} set and \(r_{max}=0\%\).\\
We therefore chose to cut the batches in half horizontally, creating $1 \times 2$ batches. This decision led to a balanced 50-50 distribution between \textbf{src} and \textbf{dst}.
\begin{align*}
    r_{max} = 1-\frac{1}{1*2} = 50\%
\end{align*}
This limits our merge volume to 50\% but allows for better matches during the merge process. Compared to the $2 \times 2$ default, every \textbf{src} token now has double the number of \textbf{dst} tokens to be matched with.\\
\begin{figure}[!htb]
    \input{plots/plot6_fid}
    \input{plots/plot6_time}
\caption{FID and relative time compared to r=0\% for 3)}
\label{fig:exp_3}
\end{figure}\\
Resizing the batches only has minor effects on image generation speed, as all three configurations consistently remain within one second of each other, although the setup with $3 \times 3$ batch does consistently perform the best in that regard (see Tab.~\ref{table:exp_3}).\\
Image quality shows a noticeable improvement with smaller batch sizes. FID values for the ToMe setup using $3 \times 3$ batches slightly exceed the ones of the setup with $2 \times 2$ batches when \(r>10\%\), which in turn clearly exceed the ones of the version with $1 \times 2$ batches. (see Fig.~\ref{fig:exp_3}).
The gap closes towards \(r=50\%=r_{max}\), as every \textbf{src} token has to be merged when using $1 \times 2$ batches, regardless of how good of a match from \textbf{dst} is available.\\
It can still be concluded that using the $1 \times 2$ batches to create an equal number of \textbf{src} and \textbf{dst} tokens yields the best performance (even at a minor speed discount), especially when \(r\) is not close to \(0\%\) or \(r_{max}\), while the usage of larger batches with a smaller \(\textbf{dst\%}\) is not advisable.



%\newpage
\subsubsection*{4): Putting it all together}
Finally, we are comparing the most successful configurations from the past experiments. That means we are looking at \textbf{setup 1}: the ToMe default by Bolya and Hofmann (black), \textbf{setup 2}: our new default with token merging expanded to the cross-attn layer from \(1.2\) and \(1.4\) (red), and \textbf{setup 3}: the new default but with $1 \times 2$ batches for token partitioning from \(3.2\) (blue).\\
We expanded the image sets for the first two configurations to 1000 images per merge volume in \(1.4\), so we will do the same for the other one to enable examinations on a larger scale.
\begin{figure}[!htb]
    \input{plots/plot7_fid}
    \input{plots/plot7_time}
\caption{FID and relative time compared to r=0\% for 4)}
\label{fig:exp_4}
\end{figure}\\
Speed is very close across the board, with the speedup offered by every version always differing by less than 1.2\%. Within these thin margins, \textbf{Setup 2} consistently beat \textbf{Setup 1} when \(r>30\%\) and \textbf{Setup 3} was the quickest of the three most of the time (see Tab.~\ref{table:exp_4}).
Our discoveries regarding image quality are reinforced, with \textbf{setup 3} noticeably outperforming \textbf{setup 2}, which in turn performs slightly better than \textbf{setup 1} (see Fig.~\ref{fig:exp_4}).\\



%\newpage
\subsubsection*{Summary}
The experiments we conducted suggest that the best performance can be achieved by applying token merging in both the self-attn and cross-attn layer of the transformer. Depending on whether speed or image quality is the most important demand, $1 \times 2$ (best image quality) or $2 \times 2$ batches (highest speed) can be chosen for token partitioning, with $1 \times 2$ batches bringing particularly great improvements to image quality when \(r\leq40\%\).\\
Another important discovery is that token merging in the self-attn module is essential to unlocking speed improvements for image generation. On the other hand, using ToMe in the mlp layer seems to exclusively come at a disadvantage.\\
Moreover, it is important to create sufficiently large images when using ToMe in order to see the speed benefits. We showed that the time improvements with token merging noticeably decrease from >50\% for $768 \times 768$ images to only about 25\% for $512 \times 512$ images at \(r=60\%\) (see Tab.~\ref{table:exp_1_4},~\ref{table:exp_2}).\\
Additionally, it was shown that strongly skewing the \textbf{src}-\textbf{dst}-ratio away from an equal distribution has negative effects on image quality as well (see Tab.~\ref{table:exp_2}).
