\section{Related Work}


\subsection*{Token Pruning}
Several works have tried to utilize the transformer's flexibility when it comes to handling inconsistent amounts of tokens by pruning the tokens that are considered the least important \cite{meng2022adavit,yin2022vit}. These methods, unlike ToMe, firstly require specific training for each model and secondly dynamically determine the number of pruned tokens which leads to variances between images. The fluctuations in token number improve accuracy, but make batching of different samples impossible and often require the use of masks for training. This eradicates or at least limits the speed-up that can be gained during training.\\
ToMe does not suffer from these problems as the number of tokens always stays the same outside of transformer components and no training is required. Still, ToMe achieves speed-ups in both image generation and training of models.



\subsection*{Combining Tokens}
Other works try to combine tokens instead of pruning them.
There have been different approaches ranging from fusing less informative tokens into a single package token \cite{kong2021spvit, liang2022not}, over using an mlp with spatial attention for token reduction \cite{ryoo2021tokenlearner}, to using a learned deformable token merging module for adaptive token merging between stages \cite{pan2022less}.\\
The most similar method to ToMe is Token Pooling by \cite{marin2021token}, though it does again require specific finetuning. Token Pooling reduces the number of tokens to \(K\) by using k-means to cluster the tokens into \(K\) clusters. In this case the cluster centers represent the new tokens.\\
"Until now, no approach has been successful in offering a reasonable speed-accuracy trade-off when combining tokens without training" \cite{bolya2023tome}.



\subsection*{Efficient Transformers?}