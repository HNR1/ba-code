\section{Related Work} \label{related_work}


\subsection{Token Pruning}
Several works have tried to utilize the transformer's flexibility when it comes to handling inconsistent amounts of tokens by pruning tokens that are considered the least important \cite{meng2022adavit,yin2022vit}. 
Differing from ToMe, these methods require model-specific training. Additionally, they dynamically determine the number of pruned tokens, resulting in variances between images. The flexibility in token number may improve accuracy, but makes batching of different samples impossible and requires the use of masks for training. This eradicates, or at least limits, the speedup that can be gained during training.\\
ToMe does not suffer from these problems as the number of tokens always stays the same outside of transformer components and generally no training is required. Still, ToMe achieves speedups in both image generation and training of models \cite{bolya2023tomesd}.



\subsection{Combining Tokens}
Other works try to combine tokens instead of pruning them.
There have been different approaches ranging from fusing less informative tokens into a single package token \cite{kong2021spvit, liang2022not}, over using a multi-layer perceptron with spatial attention for token reduction \cite{ryoo2021tokenlearner}, to using a learned deformable token merging module for adaptive token merging between stages \cite{pan2022less}.\\
The most similar method to ToMe is Token Pooling by \cite{marin2021token}, though it does again require specific finetuning. Token Pooling reduces the number of tokens to \(K\) by using k-means to cluster the tokens into \(K\) clusters. In this case the cluster centers represent the new tokens. The k-means-algorithm is slow, therefore Token Pooling offers a bad speed-accuracy trade-off.\\
ToMe is the only algorithm that offers reasonable speed and accuracy, without requiring any training \cite{bolya2023tome}.



%\subsection*{Efficient Transformers?}