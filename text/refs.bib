@article{bolya2023tomesd,
  title={{Token Merging for Fast Stable Diffusion}},
  author={Bolya, Daniel and Hoffman, Judy},
  journal={CVPR Workshop on Efficient Deep Learning for Computer Vision},
  year={2023}
}

@inproceedings{bolya2023tome,
  title={Token Merging: Your {ViT} but Faster},
  author={Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@misc{Seitzer2020FID,
  author={Maximilian Seitzer},
  title={{pytorch-fid: FID Score for PyTorch}},
  month={August},
  year={2020},
  note={Version 0.3.0},
  howpublished={\url{https://github.com/mseitzer/pytorch-fid}}
}

@InProceedings{Rombach_2022_CVPR,
    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10684-10695}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{liu2022pseudo,
  title={Pseudo numerical methods for diffusion models on manifolds},
  author={Liu, Luping and Ren, Yi and Lin, Zhijie and Zhao, Zhou},
  journal={arXiv preprint arXiv:2202.09778},
  year={2022}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@inproceedings{goyal2020power,
  title={PoWER-BERT: Accelerating BERT inference via progressive word-vector elimination},
  author={Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish},
  booktitle={International Conference on Machine Learning},
  pages={3690--3699},
  year={2020},
  organization={PMLR}
}

@article{kim2020length,
  title={Length-adaptive transformer: Train once with length drop, use anytime with search},
  author={Kim, Gyuwan and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:2010.07003},
  year={2020}
}

@inproceedings{kim2022learned,
  title={Learned token pruning for transformers},
  author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={784--794},
  year={2022}
}

@article{lassance2021study,
  title={A study on token pruning for colbert},
  author={Lassance, Carlos and Maachou, Maroua and Park, Joohee and Clinchant, St{\'e}phane},
  journal={arXiv preprint arXiv:2112.06540},
  year={2021}
}

@inproceedings{meng2022adavit,
  title={Adavit: Adaptive vision transformers for efficient image recognition},
  author={Meng, Lingchen and Li, Hengduo and Chen, Bor-Chun and Lan, Shiyi and Wu, Zuxuan and Jiang, Yu-Gang and Lim, Ser-Nam},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12309--12318},
  year={2022}
}

@inproceedings{yin2022vit,
  title={A-vit: Adaptive tokens for efficient vision transformer},
  author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10809--10818},
  year={2022}
}

@article{kong2021spvit,
  title={Spvit: Enabling faster vision transformers via soft token pruning},
  author={Kong, Zhenglun and Dong, Peiyan and Ma, Xiaolong and Meng, Xin and Sun, Mengshu and Niu, Wei and Shen, Xuan and Yuan, Geng and Ren, Bin and Qin, Minghai and others},
  journal={arXiv preprint arXiv:2112.13890},
  year={2021}
}

@article{song2022cp,
  title={Cp-vit: Cascade vision transformer pruning via progressive sparsity prediction},
  author={Song, Zhuoran and Xu, Yihong and He, Zhezhi and Jiang, Li and Jing, Naifeng and Liang, Xiaoyao},
  journal={arXiv preprint arXiv:2203.04570},
  year={2022}
}

@article{rao2021dynamicvit,
  title={Dynamicvit: Efficient vision transformers with dynamic token sparsification},
  author={Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13937--13949},
  year={2021}
}

@inproceedings{fayyaz2022adaptive,
  title={Adaptive token sampling for efficient vision transformers},
  author={Fayyaz, Mohsen and Koohpayegani, Soroush Abbasi and Jafari, Farnoush Rezaei and Sengupta, Sunando and Joze, Hamid Reza Vaezi and Sommerlade, Eric and Pirsiavash, Hamed and Gall, J{\"u}rgen},
  booktitle={European Conference on Computer Vision},
  pages={396--414},
  year={2022},
  organization={Springer}
}

@article{yu2023unified,
  title={A unified pruning framework for vision transformers},
  author={Yu, Hao and Wu, Jianxin},
  journal={Science China Information Sciences},
  volume={66},
  number={7},
  pages={1--2},
  year={2023},
  publisher={Springer}
}

@article{liang2022not,
  title={Not all patches are what you need: Expediting vision transformers via token reorganizations},
  author={Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  journal={arXiv preprint arXiv:2202.07800},
  year={2022}
}

@inproceedings{xu2022groupvit,
  title={Groupvit: Semantic segmentation emerges from text supervision},
  author={Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18134--18144},
  year={2022}
}

@article{ryoo2021tokenlearner,
  title={Tokenlearner: Adaptive space-time tokenization for videos},
  author={Ryoo, Michael and Piergiovanni, AJ and Arnab, Anurag and Dehghani, Mostafa and Angelova, Anelia},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12786--12797},
  year={2021}
}

@inproceedings{pan2022less,
  title={Less is more: Pay less attention in vision transformers},
  author={Pan, Zizheng and Zhuang, Bohan and He, Haoyu and Liu, Jing and Cai, Jianfei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={2},
  pages={2035--2043},
  year={2022}
}

@article{marin2021token,
  title={Token pooling in vision transformers},
  author={Marin, Dmitrii and Chang, Jen-Hao Rick and Ranjan, Anurag and Prabhu, Anish and Rastegari, Mohammad and Tuzel, Oncel},
  journal={arXiv preprint arXiv:2110.03860},
  year={2021}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{patil2022stable,
  author = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von Platen, Patrick},
  title = {Stable Diffusion with Diffusers},
  journal = {Hugging Face Blog},
  year = {2022},    
  note = {\url{https://huggingface.co/blog/stable\_diffusion}}
}

@misc{johnson2019attn,
  title={Lecture 13: Attention},
  author = {Johnson, Justin},
  howpublished = "\url{https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture13.pdf}",
  year = {2019},    
  note = {From the course \href{https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/}{Deep Learning for Computer Vision}}
}